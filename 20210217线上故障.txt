记录一次线上故障排查

背景：连续两天线上出现CPU突增，导致业务无法提供服务的情况。

调查分析

一、从监控看 1、SLB流量、并发数也有突增（怀疑有流量增加） 2、MYSQL、REDIS CPU也有明显突增，MYSQL突增到了50%的CPU （怀疑是MYSQL慢查询导致所有业务阻塞） 3、ECS出现CPU突增，时间点跟其他服务时间点非常接近，因为有HTTP服务、DUBBO RPC服务（所以怀疑是不是内部访问导致业务压力过大引起的突增）

二、从业务上面看 1、通过业务上游引用查看，有出现延迟较长的接口访问，上游调用分几个方面：统计业务、对外业务、内部调用 2、业务晚上8-24点之间是高峰期，加上刚好是春节假期，业务访问量是持续增长的，前几天没有出现故障，假期最后一天出现故障，（怀疑是业务增长引起）


解决思路和过程

1、看监控有流量突增现象，MYSQL有CPU高涨现象、ECS有CPU高涨现象，第一想法，慢SQL导致的服务卡顿。去看了RDS的慢SQL日志发现没有。

  于是去ECS看日志，看是否可以从日志发现问题，在日志中，看到大量的访问某个表的语句，没有使用缓存。查看了一下表，发现没有使用索引，但是该表的数据量只有4条，无需添加索引。
2、在故障发生之前，就发现某些机器的CPU使用率，是其他机器使用率的两倍，开始怀疑是宿主机老化问题，于是联系了阿里同事想更换宿主机。但是最终定位不是宿主机问题。

  问题在于日志收集filebeat进程占用了很大的资源。占用高的原因初步分析可能是收集了很多的日志（目前是4-5个日志），可以减少收集日志的数量（1-2个），目前还未验证。
  关闭filebeat之后，机器CPU恢复正常水位线。
3、关闭filebeat之后，第二天故障再现。另外一个服务出现CPU问题，发现另外一个服务也是有filebeat，关闭filebeat，但是发现CPU有周期性高峰。

4、通过监控看到RPC调用接口有300-400毫秒的调用，根据RPC调用时间，发现会出现CPU增长的现象，开始怀疑就是这些RPC调用引起的。但是有个现象，监控上面没过两个小时会有一个CPU增长，

  开始怀疑我们内部有定时任务，确认内部业务是没有定时任务的，询问上游服务的同事，并不存在定时任务。
5、通过Arthas监控线上服务，在高峰时段，并没有使用率特别高的线程出现。（以为自己抓取时机不对）

6、登录服务器，TOP观察CPU使用情况，发现一些自己不认识的进程。等待高峰时间，在高峰时间突然有个进程使用CPU 100%，直接占用了一个CPU资源。截图给运维，定位到是Salt-minion服务，

  该服务是用来推送服务器配置。该服务运维有计划任务，刚还是两小时一次。关闭该服务的计划任务。
7、第三天验证服务正常

总结

  本次故障引起线上业务无法正常提供服务，通过现象排查了数据库、ECS日志并没有发现异常，查看主机CPU使用情况发现filebeat占用资源，关闭filebeat并没有解决问题，再次定位到Salt-minion服务的计划任务引起的周期性问题，两小时一次，刚好在夜晚十点多有一次同步任务导致线上故障。
